---
title: "EEMB146 Lab 8 Exercise"
author: "Written by Caroline Owens, edited by Samantha Sambado"
date: "2024-01-02"
output: 
    html_document:
      toc: true
      toc_float: true
---

## Exercise 8 -- Multiple Linear Regression & Model Selection

# Learning Objectives

+ Know the assumptions of a multiple regression model
  + no collinearity between predictor Xs
  + residuals of the model are normal
  + residuals of the model have equal variance
+ Be familiar with different types of multiple regression models
  + only fixed effects multiple regression models (y ~ X1 + X2 + X3)
  + mixed-effect models (y ~ X1 + 1|X2)
  + interaction models (y ~ X1*X2)
+ Know how to evaluate AIC/BIC scores
  + generally looking for overall smallest AIC score
  + BIC penalizes many paramaters (ie predictive Xs)
  

# Background 

## Multivariate Linear Models

At this point in the quarter, you know several ways of analyzing the relationship between two variables - that is, whether one variable is *predicted by* another. We learned last week that a linear model with one predictor variable can be written as $$Y_i = \beta_0+\beta_1X_i+\epsilon_i$$ where $Y_i$ is the value of your response variable, $X_i$ is the value of your predictor variable, and $\epsilon_i$ is the value of the residual for that point. 

In real life, however, one predictor can rarely explain all of the patterns in your response variable. More often, a response can be predicted by a combination of predictors. For example, your height may be predicted by some combination of your parents' height, your diet and nutrition, your age, and other factors.
We write these multivariate linear models as 

$$Y = \beta_0+\beta_1X_1+ \beta_2X_2 + ... + \beta_nX_n+\epsilon$$

Notice that you can have as many predictors (X-variables) as you want, and that each one can be more or less important to the final value of Y depending on the relative size of its coefficient. Our height example might look like this:

$$Adult~height = (0.4)*(Mom's~height) + (0.4)*(Dad's~height)+2*(nutrition~score)+\epsilon$$

Our hope is that by adding predictors, we will be able to explain more and more of the variability in the response variable in the model, increasing our $R^2$ value and decreasing the size of the residuals. However, each additional predictor adds another coefficient that must be estimated to fit our model. Estimating these extra parameters decreases the degrees of freedom and decreases the power of the analysis. For this reason, you can never have more predictors than you have data points (i.e. your dataset should be longer than it is wide). You will get the best model performance when you have *many* more data points than predictors.

When assessing the fit of a multiple regression model, we always use the *adjusted $R^2$* value instead of the regular $R^2$. The adjustment takes into account how many parameters have been estimated to build the model, as well as the sample size. 


### Assumptions of multiple regression

You can think of a single variable linear model (LM) as a multiple regression model with all coefficients $B_n,~n>1$ set to 0, so all the assumptions from our one-variable linear model still apply to multiple regressions. However, we also add some new assumptions to handle possible interactions between the x-variables. One important new assumption is that no two predictors completely explain each other. 
A good way to check this is by using the pairs.panels() function from the psych package again. We use the scatter plots to look for variables that have a high correlation with each other. We are hoping that our predictors will have a linear relationship with our response variable (and we can transform our predictors to improve that fit if we need to). However, if we see any strong linear patterns between variables, we need to drop one of those predictors from the model. 

**Clarification: not all of your predictors need to be normally distributed, as long as the residuals of your model are normal.**

To summarize our assumptions for a general linear model, before conducting multiple regression we have to check that:

  + observations represent a random sample
  
  + there is no collinearity between predictors
  
  + there is some linear relationship between predictors and the response

  + residuals of the model are normal 

  + residuals of the model have equal variance
  

### Types of multiple regression

A)  *ANCOVA* includes a categorical (group) variable as well as a continuous (regression) variable in the set of predictors. One key assumption of ANCOVA is that within each group, the slope (relationship between continuous predictor and response) is the same.

B) If you think that the slope varies depending on the value of other predictors, you need to fit a model with *interaction terms*. lm(y~x1*x2) predicts y based on x1, x2, and the interaction of x1 and x2.

C) A variable that doesn't have a linear relationship with your response, but may be affecting its value, can be included as a *random* or *block* effect. This is a way of accounting for confounding variables, or groups that are outside your experimental design but still could affect the value of your response. To include random effects in your model, you will need to use the function lme() instead of lm(). The syntax of a model with random effects is lme(y~x1 + x2, random = ~1|g1, data = dataset). These models will often fail to run if your dataset includes NA values, so be sure to clean the dataset using na.omit() first.

### Assessing Model Fit 

In lecture, we discussed how to calculate Aikake's Information Criterion (AIC) from the log-likelihood of a model. Remember that AIC is only a valid comparison between models that were fitted using exactly the same data. The absolute value of the AIC doesn't matter; we are only concerned about the *relative* value of AIC. A model with a smaller AIC value performs relatively better than a model with a larger AIC value. Don't get tripped up by negative AICs - **we are looking for overall smallest, not closest to 0**.

We can also compare models using the BIC. AIC and BIC should generally lead us to similar results, but BIC places a higher penalty on extra parameters. This helps to enforce the principle of parsimony - that a simpler model is best.

Finally, we can compare the adjusted R-squared value for each model to see how much variation in the y-variable is explained by each combination of predictors. We are looking for a model with high adjusted R-squared.

For your homework go off of the AIC/BIC/$R^2$, but for your final project we will require you to bring some biological intuition of what model makes the most sense while adhering to the rules of parsimony. Based on the mentioned criteria, model 'fit_3var' is the best model due to the lowest AIC and highest $R^2$. 


```{r setup, include=FALSE}
## setting up the style of your knitted document
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)

################################################################################
## installing packages
library(readr) # package for reading in data
library(tidyverse) # package for data wrangling 
library(ggplot2) # package for plotting
library(car) # package for qqPlots
library(psych) # package for pairs.panels()
# install.packages("kableExtra") # NEW package to make beautiful tables
library(kableExtra) 
# install.packages("nlme") # NEW package for fitting mixed effects models (linear models with random effects) 
library(nlme) 
# install.packages("janitor") # NEW package for cleaning column names
library(janitor)

################################################################################
## upload data
# recommend putting data into a folder called 'data'
airquality <- airquality %>% 
  clean_names() # makes column names to lowercase 

```

# Exercises

The `air_quality` data set is built-in R so there is nothing to install, we just cleaned the column names to lower case to be consistent with our previous coding habits. If you want to learn more about this data use the command `help(airquality)` in your console!

As a summary, daily air quality measurements were taken in New York from May to September 1973. There are 153 observations and 6 variables. Our outcome of interest is `ozone` (bc we are humans in the 21st century) and we will try to find the "best" model to predict `ozone`. By "best" model we mean a model that explains variation of `ozone` but is also a relatively simple model (ie Occam's razor)


## Exercise 1: Fit a Multivariate Model

```{r}
## Step 1. Check data
#str(airquality)

## Step 2. Check for collineaarity between Xs with pairs.panel()
pairs.panels(airquality,
             density = TRUE,
             cor = TRUE,
             lm = TRUE) # Looking at only potential Xs, wind & temp (-0.46) and temp & month (0.42) have higher correlation values, which we should keep in mind when we are building a model.


```


Fit a model with 1 predictor X. Then fit a second model using that predictor plus some others that you think will help explain more of the variation in ozone. Compare the $R^2$ and adjusted $R^2$ values for these models. Is adding predictors helpful?

```{r}
## Step 3. Build different model types (multiple regression or mixed-effects)
## build more complex models with the given dataset

# fixed effect models
fit_1var <- lm(ozone ~ solar_r,
               data = airquality)

fit_2var <- lm(ozone ~ solar_r + wind,
               data = airquality)

fit_3var <- lm(ozone ~ solar_r + wind + temp,
               data = airquality)

# mixed effect models
# with this dataset, it may not make sense to include month (or day) as fixed effects to explain ozone. How would a Tuesday biologically impact ozone variation? But maybe we think month could be a variable that has effect on other variables we measured such as temperature (ie July tends to have hotter temperatures than December). For variables we think may have an impact, but not a direct impact (like temperature), we can keep as random effects. If a model has both fixed (ie temperature) and random (ie month) variables, it is considered a "mixed-effect model".

# we will need to transform month from an integer into a factor to run as a random effect
airquality_2 <- airquality %>% # making a different dataset just to make sure we don't overwrite the OG data
  mutate(month = as.factor(month)) %>% # transform month from int to factor
  na.omit() # remove NAs; this is important for lme()

fit_mixed <- lme(ozone ~ temp, # fixed effect temp
                 ~1|month, # random effect; with month on the right side of the pipe '|', we are saying that all observations start at the same y-intercept, but each month has a different slope (eg some sharp, some flat)
                 data = airquality_2) 


## Step 4. Check model diagnostic plots
# 1 var
#par(mfrow = c(2,2))
#plot(fit_1var) 

# 2 var
#par(mfrow = c(2,2))
#plot(fit_2var)

# 3 var
#par(mfrow = c(2,2))
#plot(fit_3var) 

```

## Exercise 2: Model Selection

In lecture, we discussed how to calculate Aikake's Information Criterion (AIC) from the log-likelihood of a model. Remember that AIC is only a valid comparison between models that were fitted using exactly the same data. The absolute value of the AIC doesn't matter; we are only concerned about the *relative* value of AIC. A model with a smaller AIC value performs relatively better than a model with a larger AIC value. Don't get tripped up by negative AICs - **we are looking for overall smallest, not closest to 0**.

We can also compare models using the BIC. AIC and BIC should generally lead us to similar results, but BIC places a higher penalty on extra parameters. This helps to enforce the principle of parsimony - that a simpler model is best.

Finally, we can compare the adjusted R-squared value for each model to see how much variation in the y-variable is explained by each combination of predictors. We are looking for a model with high adjusted R-squared.

It is often convenient to print all of these values for each model as a neat table, so that your reader can compare them all at a glance. The function kable() in the package knitr allows us to print a dataframe as a table in html, pdf, or latex format (run in console, not in your markdown, to see the different formats). Look up the help documentation for kable using ?kable. You can specify left, right, or center alignment, give the number of digits to round to for the whole table or for each column individually, edit the names of your columns, and more to get the perfect beautiful table for your report.

```{r}
## Step 5. Calculate AIC of each model
# because we used slightly different date for fit_mixed (ie removed all of the NAs), we can't compare that model with the fixed effect models (fit_1var...fit_3var). We encourage you to create more mixed-effect models and compare those if you are interested in building the best possible model.
results <- AIC(fit_1var,fit_2var,fit_3var)


## Step 6. Add other metrics to the results table
models <- list(fit_1var,fit_2var,fit_3var) # make sure you keep your models in the same order here as they were when you created them in 'results_AIC'

results$BIC <- sapply(models, BIC) # add a column for BIC to the results

model_summary <- lapply(models, summary) #look up ?lapply if you have not used this function before


## Step 7. Extract relevant information from model summaries 
# we will use a for loop to easily extract the R^2 and adj R^2 value for each model from its summary, and store them in new columns in the results table

for(i in 1:length(models)){ #this creates a variable i that starts with the value i=1
  results$rsq[i] <- model_summary[[i]]$r.squared #we assign the rsq value from model i to the i'th row of the column 'rsq' in the table 'results'
  results$adj_rsq[i] <- model_summary[[i]]$adj.r.squared #same for adjusted rsq
} #now we go back to the beginning of the for-loop, add 1 to the value of i, and do everything again


## Step 8. Create a nice kable with model results
# there are so so many ways to make kable tables look nicer, highly recommend checking out additional commands here: https://bookdown.org/yihui/rmarkdown-cookbook/kable.html#change-column-names
kable(results, # data you want visualized
      digits = 2, # sigfigs
      align = "c") # change alignment


## Step 9. Decide which model is the best (see below)
```

Deciding what is the "best fit" model is an important step of modeling and will require the consideration of many things such as; what model has lowest AIC? what model has highest $R^2$? what model balances the trade off between complexity and explaining enough variance? what model make actual biological sense? this sort of questioning happens every day when people are "modeling". However, we don't expect you to be experts here! For your homework go off of the AIC/BIC/$R^2$, but for your final project we will require you to bring some biological intuition of what model makes the most sense while adhering to the rules of parsimony. Based on the mentioned criteria, model 'fit_3var' is the best model due to the lowest AIC and highest $R^2$. 

## Exercise 3: Assess Model Fit

When you have chosen your best model based on AIC, BIC, adjusted $R^2$, and the principle of parsimony, you will want to check how well it fits the data. Are the residuals normally distributed? How large are the residuals? If you leave some datapoints out when you fit the model, can you use your model to predict those points accurately?

```{r}
## Step 9. Separate data into a training and testing sets
# You will separate your data into a training set (most of the data) and a test set (a few observations, or <10% of rows)

splitter <- sample(1:nrow(airquality), 15, replace = F) # pick 15 random rows from air_quality, don't replace
airqual_train <- airquality[-splitter,] # leave those rows out of training data
airqual_test <- airquality[splitter,] # use those rows to create a set of test data


## Step 10. Use the best fit model on TRAINING set
fit_3var_train <- lm(ozone ~ solar_r + wind + temp,
               data = airqual_train)

## Step 11. Use the fitted model with training data to make predictions of your Y outcome
fit_3var_train_predict <- predict(fit_3var_train, 
                                  airqual_test)


## Step 12. Visualize how your best fit model did with predictions
plot(airqual_test$ozone, pch = 1, ylab = "Ozone") # plot actual test data values
points(fit_3var_train_predict, pch = 20, col = "red") # plot the model predictions for those points
```

At a first glance, we can see that we don't have a lot of overlap of red and open circles. If we built a perfect model, we would have complete overlap of red/open circles at each index. For a very simple exercise like this, this makes sense our model didn't predict perfectly. Ozone is a very complicated biogeochemical process that is difficult to encapsulated in < 200 data points and 3 variables. Hopefully this exercise can be a framework for your final project model where you bring your biological intuition to a higher quality data set!


# Additional Resources for Final Project
For your Final Project, you will be responsible for creating a professional looking report about a biological system you investigated using statistical tests and R skills you learned throughout EEMB 146. Some additional resources that will be helpful are :

+ [build prettier kable tables](https://bookdown.org/yihui/rmarkdown-cookbook/kable.html#change-column-names)
+ [another way to visualize regression summaries](https://jtools.jacob-long.com/reference/summ.html)

# Vocab

For those who would like more visual examples of how to interpret linear model outputs and diagnostic plots, these resources will be helpful!

+ [more practice w/ interpretting lm outputs](https://feliperego.github.io/blog/2015/10/23/Interpreting-Model-Output-In-R)
+ [more practice w/ interpretting diagnostic plots for lm](https://data.library.virginia.edu/diagnostic-plots/)


## Appendix: using stepwise model selection (optional)

Model selection is a very complex problem for statisticians, and we have just scratched the surface here. There are many other ways to choose appropriate combinations of predictor variables. One example is stepwise model selection. This process starts with a 'full model' (including all possible predictors) and eliminates variables one by one, calculating AIC each time. When the AIC can't be reduced any more by taking away variables or putting them back, you have the final optimized model. You can try stepwise AIC in R:

```{r}
# install.packages("MASS") new package for stepAIC
library(MASS)

fullmodel <- lm(ozone~solar_r+wind+temp+month+day, data = airquality)
nullmodel <- lm(ozone~1, data = airquality)

stepAIC(fullmodel, scope = c(upper = fullmodel, lower = nullmodel), direction = "both")
```

How does including interactions affect this process? Do you get similar results from the automated selection as you did in the homework by using your biological intuition and comparing models by hand?

### Appendix: ozone in the news
https://www.npr.org/sections/health-shots/2020/05/19/854760999/traffic-is-way-down-due-to-lockdowns-but-air-pollution-not-so-much




### End of Exercise 8


